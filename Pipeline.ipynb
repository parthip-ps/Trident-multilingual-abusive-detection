{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d28173-5895-4418-bb7f-b8cf453cbe43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.40.2\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (0.34.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (2025.7.34)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (2.32.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from transformers==4.40.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.40.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.40.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.40.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->transformers==4.40.2) (2025.7.14)\n",
      "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m962.3 kB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m1.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.0\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.0:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.0\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.56.1\n",
      "\u001b[2K    Uninstalling transformers-4.56.1:━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.56.17m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers]0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.19.1 transformers-4.40.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"transformers==4.40.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "897bb9d7-86b2-407a-ae34-4fa4a8a8ba6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting epitran\n",
      "  Downloading epitran-1.34.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from epitran) (2025.7.34)\n",
      "Collecting panphon>=0.20 (from epitran)\n",
      "  Downloading panphon-0.22.2-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting marisa-trie (from epitran)\n",
      "  Downloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from epitran) (2.32.4)\n",
      "Collecting jamo (from epitran)\n",
      "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from panphon>=0.20->epitran) (80.9.0)\n",
      "Collecting unicodecsv (from panphon>=0.20->epitran)\n",
      "  Downloading unicodecsv-0.14.1.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from panphon>=0.20->epitran) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.20.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from panphon>=0.20->epitran) (1.26.4)\n",
      "Collecting editdistance (from panphon>=0.20->epitran)\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.9 kB)\n",
      "Collecting munkres (from panphon>=0.20->epitran)\n",
      "  Downloading munkres-1.1.4-py2.py3-none-any.whl.metadata (980 bytes)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from panphon>=0.20->epitran) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->panphon>=0.20->epitran) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->panphon>=0.20->epitran) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pandas->panphon>=0.20->epitran) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->panphon>=0.20->epitran) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->epitran) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->epitran) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->epitran) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->epitran) (2025.7.14)\n",
      "Downloading epitran-1.34.0-py3-none-any.whl (222 kB)\n",
      "Downloading panphon-0.22.2-py2.py3-none-any.whl (78 kB)\n",
      "Downloading editdistance-0.8.1-cp311-cp311-macosx_11_0_arm64.whl (79 kB)\n",
      "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Downloading marisa_trie-1.3.1-cp311-cp311-macosx_11_0_arm64.whl (158 kB)\n",
      "Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
      "Building wheels for collected packages: unicodecsv\n",
      "\u001b[33m  DEPRECATION: Building 'unicodecsv' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'unicodecsv'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for unicodecsv (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unicodecsv: filename=unicodecsv-0.14.1-py3-none-any.whl size=10789 sha256=a23c2eeec492fb2d2b61ca206887da77beda23647eaddc5dadd573a16c33dd62\n",
      "  Stored in directory: /Users/parthipps/Library/Caches/pip/wheels/ec/03/6f/d2e0162d94c0d451556fa43dd4d5531457245c34a36b41ef4a\n",
      "Successfully built unicodecsv\n",
      "Installing collected packages: unicodecsv, munkres, jamo, marisa-trie, editdistance, panphon, epitran\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [epitran]\n",
      "\u001b[1A\u001b[2KSuccessfully installed editdistance-0.8.1 epitran-1.34.0 jamo-0.4.1 marisa-trie-1.3.1 munkres-1.1.4 panphon-0.22.2 unicodecsv-0.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install epitran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50dcbe61-ee02-40d2-b3fa-a2b465f94223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: indic-transliteration in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.3.75)\n",
      "Requirement already satisfied: googletrans==4.0.0-rc1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (4.0.0rc1)\n",
      "Requirement already satisfied: g2p_en in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: httpx==0.13.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from googletrans==4.0.0-rc1) (0.13.3)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.7.14)\n",
      "Requirement already satisfied: hstspreload in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.1)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Requirement already satisfied: chardet==3.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n",
      "Requirement already satisfied: backports.functools-lru-cache in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from indic-transliteration) (2.0.0)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from indic-transliteration) (2025.7.34)\n",
      "Requirement already satisfied: typer in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from indic-transliteration) (0.19.2)\n",
      "Requirement already satisfied: toml in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from indic-transliteration) (0.10.2)\n",
      "Requirement already satisfied: roman in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from indic-transliteration) (5.1)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from g2p_en) (1.26.4)\n",
      "Requirement already satisfied: nltk>=3.2.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from g2p_en) (3.9.1)\n",
      "Requirement already satisfied: inflect>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from g2p_en) (7.5.0)\n",
      "Requirement already satisfied: distance>=0.1.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from g2p_en) (0.1.3)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from inflect>=0.3.1->g2p_en) (10.8.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from inflect>=0.3.1->g2p_en) (4.4.4)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.2.4->g2p_en) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.2.4->g2p_en) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.2.4->g2p_en) (4.67.1)\n",
      "Requirement already satisfied: typing_extensions>=4.14.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p_en) (4.14.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer->indic-transliteration) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer->indic-transliteration) (14.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer->indic-transliteration) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich>=10.11.0->typer->indic-transliteration) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer->indic-transliteration) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install indic-transliteration googletrans==4.0.0-rc1 g2p_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9df20c-9f28-40e8-9d25-10209c05775f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Basic imports\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', DEVICE)\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = 'training.xlsx'\n",
    "TEST_PATH = 'testing.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da3c740-c35c-465b-ae62-fb2f3e3a0065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (2400, 3)\n",
      "Test shape: (600, 3)\n",
      "label\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_excel(path):\n",
    "    df = pd.read_excel(path)\n",
    "    return df\n",
    "\n",
    "train_df = load_excel(TRAIN_PATH)\n",
    "test_df = load_excel(TEST_PATH)\n",
    "\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Test shape:', test_df.shape)\n",
    "\n",
    "# Check required columns\n",
    "for df in [train_df, test_df]:\n",
    "    for col in ['text', 'label', 'lang']:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f'Column `{col}` missing in dataset')\n",
    "    df.dropna(subset=['text'], inplace=True)\n",
    "    df['lang'] = df['lang'].str.lower().str.strip()\n",
    "\n",
    "# Class balance\n",
    "print(train_df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78bf3b0a-eee6-41ca-aa48-50caa67cc2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Try to import helpful libraries\n",
    "try:\n",
    "    from indic_transliteration.sanscript import transliterate as it_transliterate, SCHEMES\n",
    "    HAS_INDIC = True\n",
    "except:\n",
    "    HAS_INDIC = False\n",
    "\n",
    "try:\n",
    "    import epitran\n",
    "    HAS_EPITRAN = True\n",
    "except:\n",
    "    HAS_EPITRAN = False\n",
    "\n",
    "# Transliteration\n",
    "def transliterate_text(lang, text):\n",
    "    if lang == 'en': return text\n",
    "    if HAS_INDIC:\n",
    "        scheme_map = {'te':'telugu', 'ml':'malayalam'}\n",
    "        try:\n",
    "            return it_transliterate(text, scheme_map.get(lang,'iast'), 'iast')\n",
    "        except:\n",
    "            return text\n",
    "    import unicodedata\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# G2P mapping\n",
    "def g2p_map(lang, text):\n",
    "    if lang == 'en':\n",
    "        return ' '.join(list(text.replace(' ', '_')))\n",
    "    if HAS_EPITRAN:\n",
    "        try:\n",
    "            epi = epitran.Epitran({'te':'tel-Deva','ml':'mal-Mlym'}.get(lang,'eng-Latn'))\n",
    "            phons = epi.transliterate(text)\n",
    "            return ' '.join(list(phons))\n",
    "        except:\n",
    "            return ' '.join(list(text.replace(' ','_')))\n",
    "    else:\n",
    "        return ' '.join(list(text.replace(' ','_')))\n",
    "\n",
    "# English gloss fallback (original text if translation unavailable)\n",
    "def english_gloss(lang, text):\n",
    "    return text  # For now, placeholder (can integrate Helsinki-NLP translation if desired)\n",
    "\n",
    "# Precompute columns\n",
    "for df in [train_df, test_df]:\n",
    "    df['translit'] = df.apply(lambda r: transliterate_text(r['lang'], str(r['text'])), axis=1)\n",
    "    df['phonemes'] = df.apply(lambda r: g2p_map(r['lang'], r['translit']), axis=1)\n",
    "    df['gloss'] = df['text']  # simple fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e3457f-5447-469c-8dc0-5a6fe18fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessed data saved.\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data\n",
    "train_df.to_pickle('train_preprocessed.pkl')\n",
    "test_df.to_pickle('test_preprocessed.pkl')\n",
    "print(\"✅ Preprocessed data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2950a1-6b02-45e9-aa0b-bf045c376c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "#train_df = pd.read_pickle('train_preprocessed.pkl')\n",
    "#test_df = pd.read_pickle('test_preprocessed.pkl')\n",
    "#print(\"✅ Preprocessed data loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d2c05e-5e48-4910-9e11-cf9426561c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme vocab size: 161\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "PHONEME_VOCAB_SIZE = 5000\n",
    "\n",
    "def build_phoneme_vocab(series_phonemes, max_vocab=PHONEME_VOCAB_SIZE-2):\n",
    "    ctr = Counter()\n",
    "    for s in series_phonemes:\n",
    "        ctr.update(s.strip().split())\n",
    "    most = ctr.most_common(max_vocab)\n",
    "    idx2tok = ['<pad>','<unk>'] + [w for w,_ in most]\n",
    "    tok2idx = {t:i for i,t in enumerate(idx2tok)}\n",
    "    return tok2idx, idx2tok\n",
    "\n",
    "phon_tok2idx, phon_idx2tok = build_phoneme_vocab(train_df['phonemes'].tolist())\n",
    "PHON_VOCAB_SIZE = len(phon_tok2idx)\n",
    "print('Phoneme vocab size:', PHON_VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44680a97-f6ae-4940-9cf3-23bf3848792a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe987868685e4412ab0fd2c93094d6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "TEXT_MODEL_NAME = 'xlm-roberta-base'\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "text_config = AutoConfig.from_pretrained(TEXT_MODEL_NAME)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# Adapter module\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, hidden_size, bottleneck=256):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(hidden_size, bottleneck)\n",
    "        self.act = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck, hidden_size)\n",
    "    def forward(self, x):\n",
    "        return self.up(self.act(self.down(x)))\n",
    "\n",
    "text_adapter = Adapter(text_config.hidden_size, bottleneck=128).to(DEVICE)\n",
    "\n",
    "# Phoneme encoder\n",
    "PHONEME_EMB_DIM = 256\n",
    "PHONEME_NHEAD = 8\n",
    "PHONEME_NLAYERS = 3\n",
    "\n",
    "class PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=PHONEME_EMB_DIM, nhead=PHONEME_NHEAD, nlayers=PHONEME_NLAYERS):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.embedding(input_ids).transpose(0,1)\n",
    "        out = self.transformer(emb).transpose(0,1)\n",
    "        mask = (input_ids!=0).unsqueeze(-1).float()\n",
    "        summed = (out*mask).sum(1)\n",
    "        lengths = mask.sum(1).clamp(min=1.0)\n",
    "        return summed/lengths\n",
    "\n",
    "phoneme_encoder = PhonemeEncoder(vocab_size=PHON_VOCAB_SIZE).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cd1b582-e5b9-4088-8cde-2de2068ffa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "MAX_TEXT_LEN = 128\n",
    "MAX_PH_LEN = 128\n",
    "lang2idx = {'en':0,'te':1,'ml':2}\n",
    "\n",
    "def phoneme_tokenize(ph_str, tok2idx, max_len=MAX_PH_LEN):\n",
    "    toks = ph_str.strip().split()\n",
    "    ids = [tok2idx.get(t, tok2idx['<unk>']) for t in toks][:max_len]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [tok2idx['<pad>']]*(max_len-len(ids))\n",
    "    return ids\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class AbuseDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.df.iloc[idx]\n",
    "\n",
    "        # use 'text' instead of 'translit'\n",
    "        text = str(r['text'])\n",
    "\n",
    "        # if phonemes column exists, use it; otherwise generate or fill zeros\n",
    "        if 'phonemes' in self.df.columns:\n",
    "            phon_seq = phoneme_tokenize(r['phonemes'], phon_tok2idx)\n",
    "        else:\n",
    "            # fallback: create dummy phoneme ids of max length\n",
    "            phon_seq = [0] * MAX_TEXT_LEN  \n",
    "\n",
    "        # text encoding (same as before)\n",
    "        enc = text_tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=MAX_TEXT_LEN,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # label (convert to float for BCE, or long for CE — adjust as needed)\n",
    "        label = torch.tensor(int(r['label']), dtype=torch.float)\n",
    "\n",
    "        # language ID mapping\n",
    "        lid = torch.tensor(lang2idx.get(r['lang'], 0), dtype=torch.long)\n",
    "\n",
    "        # convert phoneme list to tensor\n",
    "        ph_ids = torch.tensor(phon_seq, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'phon_ids': ph_ids,\n",
    "            'label': label,\n",
    "            'lang_id': lid\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = AbuseDataset(train_df)\n",
    "valid_dataset = AbuseDataset(test_df)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46a00084-09c9-4953-8e89-2b9834941412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, text_dim, phon_dim, hidden_dim=512, nhead=8):\n",
    "        super().__init__()\n",
    "        self.proj_text = nn.Linear(text_dim, hidden_dim)\n",
    "        self.proj_phon = nn.Linear(phon_dim, hidden_dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads=nhead, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "    def forward(self, text_feats, phon_feats):\n",
    "        q = self.proj_text(text_feats)\n",
    "        kv = self.proj_phon(phon_feats).unsqueeze(1)\n",
    "        attn_out, _ = self.cross_attn(q, kv, kv)\n",
    "        out = self.norm(attn_out + q)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "class AbuseModel(nn.Module):\n",
    "    def __init__(self, text_model, text_adapter, phoneme_encoder, n_langs=3, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.text_adapter = text_adapter\n",
    "        self.phoneme_encoder = phoneme_encoder\n",
    "        self.fusion = CrossAttentionFusion(text_dim=text_config.hidden_size, phon_dim=PHONEME_EMB_DIM, hidden_dim=hidden_dim)\n",
    "        self.abuse_head = nn.Linear(hidden_dim, 1)\n",
    "        self.lid_head = nn.Linear(hidden_dim, n_langs)\n",
    "        self.reconstructor = nn.Sequential(nn.Linear(hidden_dim, text_config.hidden_size), nn.ReLU(), nn.Linear(text_config.hidden_size, text_config.hidden_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, phon_ids):\n",
    "        out = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last = out.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        pooled = (last*mask).sum(1)/mask.sum(1).clamp(min=1.0)\n",
    "        adapted = pooled + self.text_adapter(pooled)\n",
    "        phon_pooled = self.phoneme_encoder(phon_ids.to(pooled.device))\n",
    "        fused = self.fusion(adapted.unsqueeze(1), phon_pooled)\n",
    "        abuse_logits = self.abuse_head(fused).squeeze(-1)\n",
    "        lid_logits = self.lid_head(fused)\n",
    "        recon_text_embed = self.reconstructor(fused)\n",
    "        return {'abuse_logits': abuse_logits,'lid_logits':lid_logits,'text_embed':pooled,'fused':fused,'recon_text_embed':recon_text_embed,'phon_embed':phon_pooled}\n",
    "\n",
    "model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f303ed1-b3e5-44af-8220-822ab0849d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Focal BCE\n",
    "class FocalBCELoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def forward(self, logits, targets):\n",
    "        prob = torch.sigmoid(logits)\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = prob*targets + (1-prob)*(1-targets)\n",
    "        alpha_factor = self.alpha*targets + (1-self.alpha)*(1-targets)\n",
    "        mod_factor = (1.0 - p_t)**self.gamma\n",
    "        return (alpha_factor*mod_factor*bce).mean()\n",
    "\n",
    "focal_loss_fn = FocalBCELoss()\n",
    "ce_loss_fn = nn.CrossEntropyLoss()\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "def compute_metrics(y_true, y_pred_probs, threshold=0.5):\n",
    "    y_pred = (np.array(y_pred_probs) >= threshold).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp/(tp+fn) if tp+fn>0 else 0\n",
    "    specificity = tn/(tn+fp) if tn+fp>0 else 0\n",
    "    return {'macro_precision':precision,'macro_recall':recall,'macro_f1':f1,'accuracy':acc,'sensitivity':sensitivity,'specificity':specificity,'error_rate':1-acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe52587-342f-4e2e-bf12-f2d7035cfc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "EPOCHS = 6\n",
    "lr = 2e-5\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "total_steps = len(train_loader)*EPOCHS\n",
    "warmup_steps = int(0.06*total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97d13ffd-b449-4aca-ab53-f66825cc2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def train_one_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(loader, desc='train'):\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        phon_ids = batch['phon_ids'].to(DEVICE)\n",
    "        labels = batch['label'].to(DEVICE)\n",
    "        lids = batch['lang_id'].to(DEVICE)\n",
    "\n",
    "        out = model(input_ids, attention_mask, phon_ids)\n",
    "        loss_abuse = focal_loss_fn(out['abuse_logits'], labels)\n",
    "        loss_lid = ce_loss_fn(out['lid_logits'], lids)\n",
    "        loss_rt = mse_loss_fn(out['recon_text_embed'], out['text_embed'])\n",
    "        loss = loss_abuse + 0.5*loss_lid + 0.5*loss_rt\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "    return np.mean(losses)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [],[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='eval'):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            phon_ids = batch['phon_ids'].to(DEVICE)\n",
    "            labels = batch['label'].numpy()\n",
    "            out = model(input_ids, attention_mask, phon_ids)\n",
    "            probs = torch.sigmoid(out['abuse_logits']).cpu().numpy()\n",
    "            all_probs.extend(probs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return compute_metrics(all_labels, all_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92993cd5-ae41-40c7-ae81-468737907bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'lang'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('training.xlsx')\n",
    "print(df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b7e848a-f9c1-4608-8e83-c598dff2ceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 train: 100%|██████████████████████████| 150/150 [18:44<00:00,  7.50s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:18<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T02:22:02.697839] Epoch 1 finished in 1143.2s; train_loss=0.2770 val_f1=0.4940 acc=0.5783\n",
      "Saved checkpoint: checkpoints/epoch_1.pth\n",
      "Saved checkpoint: checkpoints/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 train: 100%|██████████████████████████| 150/150 [18:41<00:00,  7.48s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:18<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T02:41:08.500502] Epoch 2 finished in 1139.9s; train_loss=0.0946 val_f1=0.6077 acc=0.6483\n",
      "Saved checkpoint: checkpoints/epoch_2.pth\n",
      "Saved checkpoint: checkpoints/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 train: 100%|██████████████████████████| 150/150 [18:22<00:00,  7.35s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:17<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T02:59:54.835799] Epoch 3 finished in 1120.7s; train_loss=0.0595 val_f1=0.6897 acc=0.7083\n",
      "Saved checkpoint: checkpoints/epoch_3.pth\n",
      "Saved checkpoint: checkpoints/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 train: 100%|██████████████████████████| 150/150 [18:24<00:00,  7.36s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:18<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T03:18:42.562466] Epoch 4 finished in 1122.2s; train_loss=0.0555 val_f1=0.6628 acc=0.6883\n",
      "Saved checkpoint: checkpoints/epoch_4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 train: 100%|██████████████████████████| 150/150 [18:23<00:00,  7.36s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:17<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T03:37:26.593273] Epoch 5 finished in 1121.1s; train_loss=0.0639 val_f1=0.7303 acc=0.7417\n",
      "Saved checkpoint: checkpoints/epoch_5.pth\n",
      "Saved checkpoint: checkpoints/best.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 train: 100%|██████████████████████████| 150/150 [18:57<00:00,  7.58s/it]\n",
      "eval: 100%|█████████████████████████████████████| 38/38 [00:18<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-24T03:56:47.903644] Epoch 6 finished in 1155.9s; train_loss=0.0649 val_f1=0.6679 acc=0.6933\n",
      "Saved checkpoint: checkpoints/epoch_6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval: 100%|█████████████████████████████████████| 38/38 [00:18<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final val metrics: {'macro_precision': 0.7786496701044135, 'macro_recall': 0.6933333333333334, 'macro_f1': 0.6679139945374258, 'accuracy': 0.6933333333333334, 'sensitivity': 0.4166666666666667, 'specificity': 0.97, 'error_rate': 0.30666666666666664}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'phonemes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'phonemes'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 341\u001b[39m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# robustness tests\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pert \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mphon_drop\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mchar_swap\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtranslit_noise\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     rm = \u001b[43mrobustness_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mperturbation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRobustness (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpert\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m):\u001b[39m\u001b[33m\"\u001b[39m, rm)\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# run ablations\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 260\u001b[39m, in \u001b[36mrobustness_evaluation\u001b[39m\u001b[34m(model, df, perturbation, n_samples)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, r \u001b[38;5;129;01min\u001b[39;00m df_sample.iterrows():\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m perturbation == \u001b[33m'\u001b[39m\u001b[33mphon_drop\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         ph = phoneme_dropout(\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mphonemes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, drop_prob=\u001b[32m0.15\u001b[39m)\n\u001b[32m    261\u001b[39m         translit = r[\u001b[33m'\u001b[39m\u001b[33mtranslit\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# keep same transliteration\u001b[39;00m\n\u001b[32m    262\u001b[39m         \u001b[38;5;66;03m# For prediction we'll feed translit into tokenizer but phonemes as modified\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:1121\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1124\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/series.py:1237\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1236\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1237\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'phonemes'"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Utilities, checkpointing, logging, inference, adversarial routines\n",
    "# -------------------------\n",
    "import json\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(state, name='latest.pth'):\n",
    "    path = os.path.join(CHECKPOINT_DIR, name)\n",
    "    torch.save(state, path)\n",
    "    print(f\"Saved checkpoint: {path}\")\n",
    "\n",
    "def load_checkpoint(path, model, optimizer=None, scheduler=None, map_location=DEVICE):\n",
    "    checkpoint = torch.load(path, map_location=map_location)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    if optimizer and 'optim_state' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optim_state'])\n",
    "    if scheduler and 'sched_state' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['sched_state'])\n",
    "    print(f\"Loaded checkpoint from {path}\")\n",
    "    return checkpoint\n",
    "\n",
    "# Simple logger\n",
    "def save_metrics(metrics, fname=\"metrics.json\"):\n",
    "    with open(os.path.join(CHECKPOINT_DIR, fname), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "# -------------------------\n",
    "# Adversarial: FGM-style for embeddings\n",
    "# -------------------------\n",
    "class FGM:\n",
    "    \"\"\"\n",
    "    Fast Gradient Method (single-step) applied to embedding parameters.\n",
    "    We will copy and add small perturbation to embedding weights for a forward pass.\n",
    "    This supports both text model embeddings and phoneme embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=1e-3):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack_embedding(self, emb_param_name_substr='embeddings'):\n",
    "        \"\"\"\n",
    "        Adds perturbation to any parameter whose name contains emb_param_name_substr.\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_param_name_substr in name:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = self.epsilon * param.grad / (norm + 1e-8)\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "# -------------------------\n",
    "# Training + evaluation loops (with optional adversarial step)\n",
    "# -------------------------\n",
    "import datetime\n",
    "def train_loop(model, train_loader, valid_loader, optimizer, scheduler,\n",
    "               epochs=EPOCHS, early_stop_patience=3, adv_train=False,\n",
    "               adv_epsilon=1e-3, grad_clip=5.0, save_every=1):\n",
    "    best_val_f1 = -1.0\n",
    "    best_epoch = -1\n",
    "    no_improve = 0\n",
    "    metrics_history = {'train_loss':[], 'val':[]}\n",
    "    fgm = FGM(model, epsilon=adv_epsilon)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch} train\"):\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            phon_ids = batch['phon_ids'].to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "            lids = batch['lang_id'].to(DEVICE)\n",
    "\n",
    "            out = model(input_ids, attention_mask, phon_ids)\n",
    "            loss_abuse = focal_loss_fn(out['abuse_logits'], labels)\n",
    "            loss_lid = ce_loss_fn(out['lid_logits'], lids)\n",
    "            loss_rt = mse_loss_fn(out['recon_text_embed'], out['text_embed'])\n",
    "            loss = loss_abuse + 0.5*loss_lid + 0.5*loss_rt\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Optional adversarial training: single-step FGM on embedding grads\n",
    "            if adv_train:\n",
    "                # ensure grads exist\n",
    "                # attack text model embeddings and phoneme embeddings\n",
    "                fgm.attack_embedding('embeddings')   # will match huggingface embedding param names\n",
    "                # forward with perturbed embeddings\n",
    "                out_adv = model(input_ids, attention_mask, phon_ids)\n",
    "                loss_abuse_adv = focal_loss_fn(out_adv['abuse_logits'], labels)\n",
    "                loss_lid_adv = ce_loss_fn(out_adv['lid_logits'], lids)\n",
    "                loss_rt_adv = mse_loss_fn(out_adv['recon_text_embed'], out_adv['text_embed'])\n",
    "                loss_adv = loss_abuse_adv + 0.5*loss_lid_adv + 0.5*loss_rt_adv\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss_adv.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                fgm.restore()\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        metrics_history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # validation\n",
    "        val_metrics = evaluate(model, valid_loader)\n",
    "        metrics_history['val'].append(val_metrics)\n",
    "        print(f\"[{datetime.datetime.now().isoformat()}] Epoch {epoch} finished in {time.time()-t0:.1f}s; train_loss={avg_train_loss:.4f} val_f1={val_metrics['macro_f1']:.4f} acc={val_metrics['accuracy']:.4f}\")\n",
    "\n",
    "        # save checkpoint\n",
    "        if epoch % save_every == 0:\n",
    "            ck_name = f\"epoch_{epoch}.pth\"\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optim_state': optimizer.state_dict(),\n",
    "                'sched_state': scheduler.state_dict() if scheduler else None,\n",
    "                'val_metrics': val_metrics\n",
    "            }, name=ck_name)\n",
    "\n",
    "        # early stopping on macro_f1\n",
    "        if val_metrics['macro_f1'] > best_val_f1 + 1e-5:\n",
    "            best_val_f1 = val_metrics['macro_f1']\n",
    "            best_epoch = epoch\n",
    "            no_improve = 0\n",
    "            # save best\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch,\n",
    "                'model_state': model.state_dict(),\n",
    "                'optim_state': optimizer.state_dict(),\n",
    "                'sched_state': scheduler.state_dict() if scheduler else None,\n",
    "                'val_metrics': val_metrics\n",
    "            }, name='best.pth')\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (best epoch {best_epoch} f1={best_val_f1:.4f})\")\n",
    "                break\n",
    "\n",
    "    save_metrics(metrics_history, fname=\"metrics.json\")\n",
    "    return metrics_history\n",
    "\n",
    "# -------------------------\n",
    "# Inference helpers\n",
    "# -------------------------\n",
    "def preprocess_single(text, lang):\n",
    "    \"\"\"\n",
    "    Take raw text + language tag -> produce tokenized inputs similar to dataset\n",
    "    \"\"\"\n",
    "    translit = transliterate_text(lang, text)\n",
    "    phon = g2p_map(lang, translit)\n",
    "    enc = text_tokenizer(translit, truncation=True, padding='max_length', max_length=MAX_TEXT_LEN, return_tensors='pt')\n",
    "    ph_ids = torch.tensor(phoneme_tokenize(phon, phon_tok2idx), dtype=torch.long).unsqueeze(0)\n",
    "    lid = lang2idx.get(lang, 0)\n",
    "    return enc['input_ids'].squeeze(0), enc['attention_mask'].squeeze(0), ph_ids.squeeze(0), lid\n",
    "\n",
    "def predict_batch(model, texts, langs):\n",
    "    \"\"\"\n",
    "    texts: list of strings\n",
    "    langs: list of lang codes corresponding to texts\n",
    "    returns: list of dicts: {'text', 'lang', 'prob', 'label', 'lid_pred', 'lid_confidence'}\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    enc_batch = text_tokenizer([transliterate_text(l, t) for t, l in zip(texts, langs)],\n",
    "                               truncation=True, padding='longest', max_length=MAX_TEXT_LEN, return_tensors='pt')\n",
    "    ph_batch = []\n",
    "    for t, l in zip(texts, langs):\n",
    "        ph = g2p_map(l, transliterate_text(l, t))\n",
    "        ph_ids = phoneme_tokenize(ph, phon_tok2idx)\n",
    "        ph_batch.append(ph_ids)\n",
    "    ph_batch = torch.tensor(ph_batch, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        out = model(enc_batch['input_ids'].to(DEVICE), enc_batch['attention_mask'].to(DEVICE), ph_batch.to(DEVICE))\n",
    "        probs = torch.sigmoid(out['abuse_logits']).detach().cpu().numpy()\n",
    "        lids_logits = out['lid_logits'].detach().cpu().numpy()\n",
    "        lids_pred = lids_logits.argmax(axis=1)\n",
    "        lids_conf = (torch.softmax(torch.tensor(lids_logits), dim=1).max(dim=1).values).numpy()\n",
    "    for i, txt in enumerate(texts):\n",
    "        results.append({\n",
    "            'text': txt,\n",
    "            'lang': langs[i],\n",
    "            'prob': float(probs[i]),\n",
    "            'label': int(probs[i] >= 0.5),\n",
    "            'lid_pred': int(lids_pred[i]),\n",
    "            'lid_confidence': float(lids_conf[i])\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def predict_single(model, text, lang):\n",
    "    input_ids, attention_mask, ph_ids, lid = preprocess_single(text, lang)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids.unsqueeze(0).to(DEVICE),\n",
    "                    attention_mask.unsqueeze(0).to(DEVICE),\n",
    "                    ph_ids.unsqueeze(0).to(DEVICE))\n",
    "        prob = float(torch.sigmoid(out['abuse_logits']).item())\n",
    "        lid_logits = out['lid_logits'].squeeze(0).cpu().numpy()\n",
    "        lid_pred = int(lid_logits.argmax())\n",
    "        lid_conf = float(torch.softmax(torch.tensor(lid_logits), dim=0).max().item())\n",
    "    return {'text':text, 'lang':lang, 'prob':prob, 'label': int(prob>=0.5), 'lid_pred':lid_pred, 'lid_confidence':lid_conf}\n",
    "\n",
    "# -------------------------\n",
    "# Robustness test utilities (noise on phonemes / transliteration / char swaps)\n",
    "# -------------------------\n",
    "import random, re\n",
    "def phoneme_dropout(ph_str, drop_prob=0.1):\n",
    "    toks = ph_str.strip().split()\n",
    "    out = [t for t in toks if random.random() > drop_prob]\n",
    "    if len(out)==0:\n",
    "        out = toks[:1]\n",
    "    return ' '.join(out)\n",
    "\n",
    "def char_swap_noise(text, swap_prob=0.05):\n",
    "    chars = list(text)\n",
    "    for i in range(len(chars)-1):\n",
    "        if random.random() < swap_prob:\n",
    "            chars[i], chars[i+1] = chars[i+1], chars[i]\n",
    "    return ''.join(chars)\n",
    "\n",
    "def translit_noise(lang, text, noise_level=0.05):\n",
    "    # apply char swaps to transliterated text\n",
    "    t = transliterate_text(lang, text)\n",
    "    return char_swap_noise(t, swap_prob=noise_level)\n",
    "\n",
    "def robustness_evaluation(model, df, perturbation='phon_drop', n_samples=100):\n",
    "    \"\"\"\n",
    "    Run evaluation on a subset with a particular perturbation applied and return metrics.\n",
    "    perturbation: 'phon_drop', 'char_swap', 'translit_noise'\n",
    "    \"\"\"\n",
    "    # Pick n_samples random rows\n",
    "    df_sample = df.sample(min(n_samples, len(df)), random_state=SEED).reset_index(drop=True)\n",
    "    texts = []\n",
    "    langs = []\n",
    "    labels = []\n",
    "    for _, r in df_sample.iterrows():\n",
    "        if perturbation == 'phon_drop':\n",
    "            ph = phoneme_dropout(r['phonemes'], drop_prob=0.15)\n",
    "            translit = r['translit']  # keep same transliteration\n",
    "            # For prediction we'll feed translit into tokenizer but phonemes as modified\n",
    "            texts.append(translit)\n",
    "            langs.append(r['lang'])\n",
    "            labels.append(int(r['label']))\n",
    "        elif perturbation == 'char_swap':\n",
    "            t_noisy = char_swap_noise(r['text'], swap_prob=0.08)\n",
    "            texts.append(t_noisy)\n",
    "            langs.append(r['lang'])\n",
    "            labels.append(int(r['label']))\n",
    "        elif perturbation == 'translit_noise':\n",
    "            t_noisy = translit_noise(r['lang'], r['text'], noise_level=0.08)\n",
    "            texts.append(t_noisy)\n",
    "            langs.append(r['lang'])\n",
    "            labels.append(int(r['label']))\n",
    "        else:\n",
    "            texts.append(r['text'])\n",
    "            langs.append(r['lang'])\n",
    "            labels.append(int(r['label']))\n",
    "    preds = predict_batch(model, texts, langs)\n",
    "    probs = [p['prob'] for p in preds]\n",
    "    metrics = compute_metrics(labels, probs, threshold=0.5)\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# Ablation runner\n",
    "# -------------------------\n",
    "def run_ablation(ablation_name, disable_adapter=False, disable_phoneme=False, disable_reconstructor=False):\n",
    "    \"\"\"\n",
    "    ablation_name: string\n",
    "    toggles: disable parts to evaluate their effect\n",
    "    This creates a shallow copy model and modifies components to disable them.\n",
    "    \"\"\"\n",
    "    model_copy = deepcopy(model)\n",
    "    model_copy.to(DEVICE)\n",
    "    if disable_adapter:\n",
    "        # replace adapter with identity\n",
    "        model_copy.text_adapter = nn.Identity()\n",
    "        print(\"Adapter disabled for ablation\")\n",
    "    if disable_phoneme:\n",
    "        # replace phoneme encoder with zero-output module\n",
    "        class DummyPhoneme(nn.Module):\n",
    "            def __init__(self, out_dim=PHONEME_EMB_DIM):\n",
    "                super().__init__()\n",
    "                self.out_dim = out_dim\n",
    "            def forward(self, x):\n",
    "                b = x.size(0)\n",
    "                return torch.zeros((b, self.out_dim), device=x.device)\n",
    "        model_copy.phoneme_encoder = DummyPhoneme()\n",
    "        print(\"Phoneme encoder disabled for ablation\")\n",
    "    if disable_reconstructor:\n",
    "        model_copy.reconstructor = nn.Identity()\n",
    "        print(\"Reconstructor disabled for ablation\")\n",
    "\n",
    "    # quick eval\n",
    "    metrics = evaluate(model_copy, valid_loader)\n",
    "    print(f\"Ablation {ablation_name} results: {metrics}\")\n",
    "    return metrics\n",
    "\n",
    "# -------------------------\n",
    "# Example: run full training\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Re-create optimizer/scheduler if needed\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    total_steps = len(train_loader) * EPOCHS\n",
    "    warmup_steps = int(0.06 * total_steps)\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "    metrics_history = train_loop(model, train_loader, valid_loader, optimizer, scheduler,\n",
    "                                 epochs=EPOCHS, early_stop_patience=3,\n",
    "                                 adv_train=True, adv_epsilon=1e-3, save_every=1)\n",
    "\n",
    "    # final evaluation on validation/test\n",
    "    val_metrics = evaluate(model, valid_loader)\n",
    "    print(\"Final val metrics:\", val_metrics)\n",
    "\n",
    "    # robustness tests\n",
    "    for pert in ['phon_drop', 'char_swap', 'translit_noise']:\n",
    "        rm = robustness_evaluation(model, test_df, perturbation=pert, n_samples=200)\n",
    "        print(f\"Robustness ({pert}):\", rm)\n",
    "\n",
    "    # run ablations\n",
    "    ablations = {\n",
    "        'no_adapter': {'disable_adapter':True},\n",
    "        'no_phoneme': {'disable_phoneme':True},\n",
    "        'no_recon': {'disable_reconstructor':True}\n",
    "    }\n",
    "    ablation_results = {}\n",
    "    for name, opts in ablations.items():\n",
    "        res = run_ablation(name, **opts)\n",
    "        ablation_results[name] = res\n",
    "    save_metrics({'final_val': val_metrics, 'robustness': rm, 'ablations': ablation_results}, fname=\"final_results.json\")\n",
    "\n",
    "    # save tokenizer & phoneme vocab\n",
    "    text_tokenizer.save_pretrained(CHECKPOINT_DIR)\n",
    "    with open(os.path.join(CHECKPOINT_DIR, \"phon_tok2idx.json\"), \"w\") as f:\n",
    "        json.dump(phon_tok2idx, f)\n",
    "\n",
    "    print(\"Training + evaluation complete. Best model saved as checkpoints/best.pth\")\n",
    "\n",
    "# -------------------------\n",
    "# Notes & quick tips\n",
    "# -------------------------\n",
    "# - If GPU memory is tight: lower MAX_TEXT_LEN or reduce BATCH_SIZE.\n",
    "# - For stronger adversarial training, replace FGM with multi-step PGD (increase compute).\n",
    "# - For view-agreement losses: you can add contrastive loss between text_embed and phon_embed\n",
    "#   e.g., NT-Xent on pooled representations; add weights to main loss.\n",
    "# - For data augmentation: consider back-translation (Helsinki models), code-mixed insertion,\n",
    "#   or phoneme-level substitution to improve generalization.\n",
    "# - To run evaluation on a specific checkpoint:\n",
    "#     ck = torch.load('checkpoints/best.pth', map_location=DEVICE)\n",
    "#     model.load_state_dict(ck['model_state'])\n",
    "#     print(evaluate(model, valid_loader))\n",
    "#\n",
    "# - To produce per-class reports:\n",
    "#     y_true, y_probs = ... collect from evaluate loop and call classification_report or compute_metrics with thresholds\n",
    "#\n",
    "# Hyperparameter hints:\n",
    "# - Epsilon for FGM: 1e-3 or 1e-2 (too large -> unstable)\n",
    "# - FocalLoss alpha/gamma tuneable per class imbalance (alpha ~ 0.25, gamma 2.0 is a good start)\n",
    "# - Use gradient accumulation if batch size too small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "698a8f23-5db0-4d14-8dd6-cbcf62c1e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved as trident_max.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model weights\n",
    "torch.save(model.state_dict(), \"trident_max.pt\")\n",
    "print(\"Model weights saved as trident_max.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "884fdaea-4a85-45a1-adeb-f437602249c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf35217e7ce417d9116c1804cb9a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perturbation: phon_drop:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness (phon_drop): {'accuracy': 0.67, 'macro_precision': 0.7662079257171281, 'macro_recall': 0.6606946251626464, 'macro_f1': 0.6296711929076422}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb01b66f12340d2a983d534c7302de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perturbation: char_swap:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness (char_swap): {'accuracy': 0.65, 'macro_precision': 0.7165209387942936, 'macro_recall': 0.6412771494344911, 'macro_f1': 0.612789025334661}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55e284230c4ffa918771c780157865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Perturbation: translit_noise:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robustness (translit_noise): {'accuracy': 0.64, 'macro_precision': 0.7255411255411255, 'macro_recall': 0.6303673305975378, 'macro_f1': 0.592944369063772}\n",
      "{'text': 'നീ very smart!', 'predicted_language': 'te', 'abuse_prediction': 'Not Abusive', 'abuse_prob': 0.27650994062423706}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Load and fix test dataset\n",
    "# -----------------------------\n",
    "TEST_PATH = 'testing.xlsx'\n",
    "test_df = pd.read_excel(TEST_PATH)\n",
    "\n",
    "# Standardize columns\n",
    "test_df.dropna(subset=['text'], inplace=True)\n",
    "test_df['lang'] = test_df['lang'].str.lower().str.strip()\n",
    "test_df.rename(columns={'phoneme':'phonemes'}, inplace=True)\n",
    "test_df['lang_id'] = test_df['lang'].map(lang2idx)\n",
    "\n",
    "# Precompute translit, phonemes, gloss if not already\n",
    "for df in [test_df]:\n",
    "    df['translit'] = df.apply(lambda r: transliterate_text(r['lang'], str(r['text'])), axis=1)\n",
    "    df['phonemes'] = df.apply(lambda r: g2p_map(r['lang'], r['translit']), axis=1)\n",
    "    df['gloss'] = df['text']\n",
    "\n",
    "# -----------------------------\n",
    "# Load saved model\n",
    "# -----------------------------\n",
    "model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"trident_max.pt\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# -----------------------------\n",
    "# Perturbation functions\n",
    "# -----------------------------\n",
    "def phoneme_dropout(ph_str, drop_prob=0.15):\n",
    "    toks = ph_str.strip().split()\n",
    "    new_toks = [t for t in toks if random.random() > drop_prob]\n",
    "    return ' '.join(new_toks) if new_toks else ' '.join(toks)\n",
    "\n",
    "def char_swap(text, swap_prob=0.1):\n",
    "    text = list(text)\n",
    "    for i in range(len(text)-1):\n",
    "        if random.random() < swap_prob:\n",
    "            text[i], text[i+1] = text[i+1], text[i]\n",
    "    return ''.join(text)\n",
    "\n",
    "def translit_noise(text, noise_prob=0.1):\n",
    "    text = list(text)\n",
    "    for i in range(len(text)):\n",
    "        if random.random() < noise_prob:\n",
    "            text[i] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "    return ''.join(text)\n",
    "\n",
    "# -----------------------------\n",
    "# Robustness evaluation\n",
    "# -----------------------------\n",
    "def robustness_evaluation(model, df, perturbation='phon_drop', n_samples=None):\n",
    "    model.eval()\n",
    "    if n_samples:\n",
    "        df_sample = df.sample(n_samples, random_state=42)\n",
    "    else:\n",
    "        df_sample = df\n",
    "\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    for _, r in tqdm(df_sample.iterrows(), total=len(df_sample), desc=f'Perturbation: {perturbation}'):\n",
    "        text = r['text']\n",
    "        translit = r['translit']\n",
    "        phon = r['phonemes']\n",
    "\n",
    "        if perturbation == 'phon_drop':\n",
    "            phon_mod = phoneme_dropout(phon)\n",
    "            translit_mod = translit\n",
    "        elif perturbation == 'char_swap':\n",
    "            translit_mod = char_swap(translit)\n",
    "            phon_mod = phon\n",
    "        elif perturbation == 'translit_noise':\n",
    "            translit_mod = translit_noise(translit)\n",
    "            phon_mod = phon\n",
    "        else:\n",
    "            translit_mod = translit\n",
    "            phon_mod = phon\n",
    "\n",
    "        enc = text_tokenizer(translit_mod, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].to(DEVICE)\n",
    "        attention_mask = enc['attention_mask'].to(DEVICE)\n",
    "        phon_ids = torch.tensor([phoneme_tokenize(phon_mod, phon_tok2idx)], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask, phon_ids=phon_ids)\n",
    "            abuse_prob = torch.sigmoid(out['abuse_logits']).item()\n",
    "            pred_label = int(abuse_prob >= 0.5)\n",
    "\n",
    "        all_labels.append(int(r['label']))\n",
    "        all_preds.append(pred_label)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    return {'accuracy': acc, 'macro_precision': precision, 'macro_recall': recall, 'macro_f1': f1}\n",
    "\n",
    "# Run robustness evaluation\n",
    "perturbations = ['phon_drop', 'char_swap', 'translit_noise']\n",
    "for pert in perturbations:\n",
    "    metrics = robustness_evaluation(model, test_df, perturbation=pert, n_samples=200)\n",
    "    print(f\"Robustness ({pert}):\", metrics)\n",
    "\n",
    "# -----------------------------\n",
    "# Single-text inference\n",
    "# -----------------------------\n",
    "def predict_text(text, lang='en'):\n",
    "    translit = transliterate_text(lang, text)\n",
    "    phon = g2p_map(lang, translit)\n",
    "\n",
    "    enc = text_tokenizer(translit, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    input_ids = enc['input_ids'].to(DEVICE)\n",
    "    attention_mask = enc['attention_mask'].to(DEVICE)\n",
    "    phon_ids = torch.tensor([phoneme_tokenize(phon, phon_tok2idx)], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attention_mask, phon_ids=phon_ids)\n",
    "        abuse_prob = torch.sigmoid(out['abuse_logits']).item()\n",
    "        pred_label = 'Abusive' if abuse_prob >= 0.5 else 'Not Abusive'\n",
    "        lid_idx = torch.argmax(out['lid_logits'], dim=1).item()\n",
    "        lang_pred = list(lang2idx.keys())[lid_idx]\n",
    "\n",
    "    return {'text': text, 'predicted_language': lang_pred, 'abuse_prediction': pred_label, 'abuse_prob': abuse_prob}\n",
    "\n",
    "# Example usage\n",
    "example = \"നീ very smart!\"\n",
    "result = predict_text(example, lang='ml')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7301422f-0ae2-4e30-b19b-fc0de6e11b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['text', 'label', 'lang', 'lang_id', 'translit', 'phonemes', 'gloss'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(test_df.columns)\n",
    "# Index(['text', 'label', 'lang', 'translit', 'phonemes', 'gloss'], dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07ba35d9-c703-4d4b-b458-1ce67cf79710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trident_max model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the same model architecture\n",
    "loaded_model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n",
    "\n",
    "# Load weights\n",
    "loaded_model.load_state_dict(torch.load(\"trident_max.pt\", map_location=DEVICE))\n",
    "loaded_model.eval()  # set to evaluation mode\n",
    "print(\"trident_max model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93a06276-9017-491f-a4cf-3381d666dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('phon_tok2idx.pkl', 'wb') as f:\n",
    "    pickle.dump(phon_tok2idx, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "904fa49b-71f5-4858-946f-9eb9e7db3296",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AbuseModel:\n\tUnexpected key(s) in state_dict: \"reconstructor.0.weight\", \"reconstructor.0.bias\", \"reconstructor.2.weight\", \"reconstructor.2.bias\". \n\tsize mismatch for text_model.embeddings.word_embeddings.weight: copying a param with shape torch.Size([250002, 768]) from checkpoint, the shape in current model is torch.Size([119547, 768]).\n\tsize mismatch for text_model.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for text_model.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 96\u001b[39m\n\u001b[32m     93\u001b[39m model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n\u001b[32m     95\u001b[39m checkpoint_path = \u001b[33m'\u001b[39m\u001b[33mtrident_max.pt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m model.eval()\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# 4️⃣ Prediction helper\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for AbuseModel:\n\tUnexpected key(s) in state_dict: \"reconstructor.0.weight\", \"reconstructor.0.bias\", \"reconstructor.2.weight\", \"reconstructor.2.bias\". \n\tsize mismatch for text_model.embeddings.word_embeddings.weight: copying a param with shape torch.Size([250002, 768]) from checkpoint, the shape in current model is torch.Size([119547, 768]).\n\tsize mismatch for text_model.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for text_model.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768])."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Device, tokenizer, phoneme vocab\n",
    "# -------------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "TEXT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "with open('phon_tok2idx.pkl','rb') as f:\n",
    "    phon_tok2idx = pickle.load(f)\n",
    "PHON_VOCAB_SIZE = len(phon_tok2idx)\n",
    "PHON_EMB_DIM = 256\n",
    "MAX_TEXT_LEN = 128\n",
    "MAX_PH_LEN = 128\n",
    "\n",
    "lang2idx = {'en':0,'te':1,'ml':2}\n",
    "idx2lang = {v:k for k,v in lang2idx.items()}\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Define classes exactly as during training\n",
    "# -------------------------------\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, hidden_size=768, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(hidden_size,bottleneck)\n",
    "        self.act = nn.ReLU()\n",
    "        self.up = nn.Linear(bottleneck,hidden_size)\n",
    "    def forward(self,x):\n",
    "        return self.up(self.act(self.down(x)))\n",
    "\n",
    "class PhonemeEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=PHON_EMB_DIM, nhead=8, nlayers=3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "    def forward(self, input_ids):\n",
    "        emb = self.embedding(input_ids).transpose(0,1)\n",
    "        out = self.transformer(emb).transpose(0,1)\n",
    "        mask = (input_ids!=0).unsqueeze(-1).float()\n",
    "        summed = (out*mask).sum(1)\n",
    "        lengths = mask.sum(1).clamp(min=1.0)\n",
    "        return summed/lengths\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    def __init__(self, text_dim=768, phon_dim=PHON_EMB_DIM, hidden_dim=512, nhead=8):\n",
    "        super().__init__()\n",
    "        self.proj_text = nn.Linear(text_dim, hidden_dim)\n",
    "        self.proj_phon = nn.Linear(phon_dim, hidden_dim)\n",
    "        self.cross_attn = nn.MultiheadAttention(hidden_dim, num_heads=nhead, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.ff = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim))\n",
    "    def forward(self, text_feats, phon_feats):\n",
    "        q = self.proj_text(text_feats)\n",
    "        kv = self.proj_phon(phon_feats).unsqueeze(1)\n",
    "        attn_out,_ = self.cross_attn(q,kv,kv)\n",
    "        out = self.norm(attn_out + q)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "class AbuseModel(nn.Module):\n",
    "    def __init__(self, text_model, text_adapter, phoneme_encoder, n_langs=3, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.text_adapter = text_adapter\n",
    "        self.phoneme_encoder = phoneme_encoder\n",
    "        self.fusion = CrossAttentionFusion(hidden_dim=hidden_dim)\n",
    "        self.abuse_head = nn.Linear(hidden_dim,1)\n",
    "        self.lid_head = nn.Linear(hidden_dim,n_langs)\n",
    "    def forward(self, input_ids, attention_mask, phon_ids):\n",
    "        out = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        last = out.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        pooled = (last*mask).sum(1)/mask.sum(1).clamp(min=1.0)\n",
    "        adapted = pooled + self.text_adapter(pooled)\n",
    "        phon_pooled = self.phoneme_encoder(phon_ids.to(pooled.device))\n",
    "        fused = self.fusion(adapted.unsqueeze(1), phon_pooled)\n",
    "        abuse_logits = self.abuse_head(fused).squeeze(-1)\n",
    "        lid_logits = self.lid_head(fused)\n",
    "        return {'abuse_logits': abuse_logits, 'lid_logits': lid_logits}\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Load model\n",
    "# -------------------------------\n",
    "text_adapter = Adapter(hidden_size=text_model.config.hidden_size)\n",
    "phoneme_encoder = PhonemeEncoder(PHON_VOCAB_SIZE)\n",
    "model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n",
    "\n",
    "checkpoint_path = 'trident_max.pt'\n",
    "model.load_state_dict(torch.load(checkpoint_path,map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Prediction helper\n",
    "# -------------------------------\n",
    "def phoneme_tokenize(ph_str):\n",
    "    toks = ph_str.strip().split()\n",
    "    ids = [phon_tok2idx.get(t,phon_tok2idx.get('<unk>',0)) for t in toks][:MAX_PH_LEN]\n",
    "    if len(ids)<MAX_PH_LEN:\n",
    "        ids += [phon_tok2idx.get('<pad>',0)]*(MAX_PH_LEN-len(ids))\n",
    "    return ids\n",
    "\n",
    "def predict_abuse_language(text):\n",
    "    enc = text_tokenizer(text, truncation=True, padding='max_length', max_length=MAX_TEXT_LEN, return_tensors='pt')\n",
    "    \n",
    "    # dummy phonemes: split text into chars or words; replace with actual phoneme extraction if available\n",
    "    phon_ids = torch.tensor([phoneme_tokenize(' '.join(list(text)))], dtype=torch.long)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model(enc['input_ids'].to(DEVICE),\n",
    "                    enc['attention_mask'].to(DEVICE),\n",
    "                    phon_ids.to(DEVICE))\n",
    "        abuse_prob = torch.sigmoid(out['abuse_logits']).item()\n",
    "        abuse_label = \"Abusive\" if abuse_prob>=0.5 else \"Not Abusive\"\n",
    "        lid_pred = torch.argmax(out['lid_logits'], dim=-1).item()\n",
    "        lang_pred = idx2lang.get(lid_pred,'en')\n",
    "    return {'text': text,'predicted_language': lang_pred,'abuse_prediction': abuse_label,'abuse_prob': abuse_prob}\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Test examples\n",
    "# -------------------------------\n",
    "texts = [\"നീ very smart!\",\"You are చాలా bad!\",\"This is fine\"]\n",
    "for t in texts:\n",
    "    print(predict_abuse_language(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "59a7cc1c-d5a2-419b-9330-3faedb2ec8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'നീ very smart!', 'predicted_language': 'en', 'abuse_prediction': 'Abusive', 'abuse_prob': 0.5016085505485535}\n",
      "{'text': 'You are చాలా bad!', 'predicted_language': 'te', 'abuse_prediction': 'Abusive', 'abuse_prob': 0.5065277218818665}\n",
      "{'text': 'This is fine', 'predicted_language': 'en', 'abuse_prediction': 'Not Abusive', 'abuse_prob': 0.4728836417198181}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -------------------------------\n",
    "# 0️⃣ Device\n",
    "# -------------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# -------------------------------\n",
    "# 1️⃣ Load tokenizer and phoneme vocab\n",
    "# -------------------------------\n",
    "TEXT_MODEL_NAME = 'xlm-roberta-base'\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "text_model = AutoModel.from_pretrained(TEXT_MODEL_NAME).to(DEVICE)\n",
    "\n",
    "with open('phon_tok2idx.pkl', 'rb') as f:\n",
    "    phon_tok2idx = pickle.load(f)\n",
    "\n",
    "PHON_VOCAB_SIZE = len(phon_tok2idx)\n",
    "PHON_EMB_DIM = 256  # same as training\n",
    "\n",
    "lang2idx = {'en':0, 'te':1, 'ml':2}  # update if you used different\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Define model components\n",
    "# -------------------------------\n",
    "class Adapter(torch.nn.Module):\n",
    "    def __init__(self, hidden_size, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.down = torch.nn.Linear(hidden_size, bottleneck)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.up = torch.nn.Linear(bottleneck, hidden_size)\n",
    "    def forward(self, x):\n",
    "        return self.up(self.act(self.down(x)))\n",
    "\n",
    "class PhonemeEncoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=PHON_EMB_DIM, nhead=8, nlayers=3):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=emb_dim, nhead=nhead)\n",
    "        self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=nlayers)\n",
    "    def forward(self, phon_ids):\n",
    "        emb = self.embedding(phon_ids).transpose(0,1)\n",
    "        out = self.transformer(emb).transpose(0,1)\n",
    "        mask = (phon_ids!=0).unsqueeze(-1).float()\n",
    "        pooled = (out*mask).sum(1)/mask.sum(1).clamp(min=1.0)\n",
    "        return pooled\n",
    "\n",
    "class CrossAttentionFusion(torch.nn.Module):\n",
    "    def __init__(self, text_dim=768, phon_dim=PHON_EMB_DIM, hidden_dim=512, nhead=8):\n",
    "        super().__init__()\n",
    "        self.proj_text = torch.nn.Linear(text_dim, hidden_dim)\n",
    "        self.proj_phon = torch.nn.Linear(phon_dim, hidden_dim)\n",
    "        self.cross_attn = torch.nn.MultiheadAttention(hidden_dim, num_heads=nhead, batch_first=True)\n",
    "        self.norm = torch.nn.LayerNorm(hidden_dim)\n",
    "        self.ff = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim),\n",
    "                                      torch.nn.ReLU(),\n",
    "                                      torch.nn.Linear(hidden_dim, hidden_dim))\n",
    "    def forward(self, text_feats, phon_feats):\n",
    "        q = self.proj_text(text_feats)\n",
    "        kv = self.proj_phon(phon_feats).unsqueeze(1)\n",
    "        attn_out, _ = self.cross_attn(q, kv, kv)\n",
    "        out = self.norm(attn_out + q)\n",
    "        return out.squeeze(1)\n",
    "\n",
    "class AbuseModel(torch.nn.Module):\n",
    "    def __init__(self, text_model, text_adapter, phoneme_encoder, n_langs=3, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.text_model = text_model\n",
    "        self.text_adapter = text_adapter\n",
    "        self.phoneme_encoder = phoneme_encoder\n",
    "        self.fusion = CrossAttentionFusion(hidden_dim=hidden_dim)\n",
    "        self.abuse_head = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.lid_head = torch.nn.Linear(hidden_dim, n_langs)\n",
    "        # reconstructor exists in checkpoint but optional here\n",
    "        self.reconstructor = torch.nn.Sequential(torch.nn.Linear(hidden_dim, text_model.config.hidden_size),\n",
    "                                                 torch.nn.ReLU(),\n",
    "                                                 torch.nn.Linear(text_model.config.hidden_size, text_model.config.hidden_size))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, phon_ids):\n",
    "        out = self.text_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.last_hidden_state[:,0,:] + self.text_adapter(out.last_hidden_state[:,0,:])\n",
    "        phon_pooled = self.phoneme_encoder(phon_ids.to(pooled.device))\n",
    "        fused = self.fusion(pooled.unsqueeze(1), phon_pooled)\n",
    "        abuse_logits = self.abuse_head(fused).squeeze(-1)\n",
    "        lid_logits = self.lid_head(fused)\n",
    "        return {'abuse_logits': abuse_logits, 'lid_logits': lid_logits}\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Instantiate model\n",
    "# -------------------------------\n",
    "text_adapter = Adapter(hidden_size=text_model.config.hidden_size)\n",
    "phoneme_encoder = PhonemeEncoder(PHON_VOCAB_SIZE)\n",
    "model = AbuseModel(text_model, text_adapter, phoneme_encoder).to(DEVICE)\n",
    "\n",
    "# Load checkpoint with strict=False to ignore extra keys like reconstructor\n",
    "checkpoint = torch.load('trident_max.pt', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint, strict=False)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Helper functions\n",
    "# -------------------------------\n",
    "MAX_PH_LEN = 128\n",
    "\n",
    "def phoneme_tokenize(ph_str):\n",
    "    toks = ph_str.strip().split()\n",
    "    ids = [phon_tok2idx.get(t, phon_tok2idx.get('<unk>',0)) for t in toks][:MAX_PH_LEN]\n",
    "    if len(ids) < MAX_PH_LEN:\n",
    "        ids += [phon_tok2idx.get('<pad>',0)]*(MAX_PH_LEN-len(ids))\n",
    "    return ids\n",
    "\n",
    "def predict_abuse_language(text):\n",
    "    # --- Example phoneme extraction ---\n",
    "    phonemes = text.split()  # Replace with your phoneme extraction logic\n",
    "    enc = text_tokenizer(text, truncation=True, padding='max_length', max_length=128, return_tensors='pt')\n",
    "    ph_ids = torch.tensor([phoneme_tokenize(\" \".join(phonemes))], dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(enc['input_ids'].to(DEVICE),\n",
    "                    enc['attention_mask'].to(DEVICE),\n",
    "                    ph_ids.to(DEVICE))\n",
    "        abuse_prob = torch.sigmoid(out['abuse_logits']).item()\n",
    "        abuse_pred = \"Abusive\" if abuse_prob>0.5 else \"Not Abusive\"\n",
    "        lid_pred = torch.argmax(out['lid_logits'], dim=-1).item()\n",
    "        lang_pred = [k for k,v in lang2idx.items() if v==lid_pred][0]\n",
    "\n",
    "    return {\n",
    "        'text': text,\n",
    "        'predicted_language': lang_pred,\n",
    "        'abuse_prediction': abuse_pred,\n",
    "        'abuse_prob': abuse_prob\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 5️⃣ Test\n",
    "# -------------------------------\n",
    "texts = [\n",
    "    \"നീ very smart!\", \n",
    "    \"You are చాలా bad!\", \n",
    "    \"This is fine\"\n",
    "]\n",
    "for t in texts:\n",
    "    print(predict_abuse_language(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbcb2c02-e5f0-42c8-bde3-b8e047e29531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Language Detection Metrics =====\n",
      "Accuracy: 0.9616666666666667\n",
      "Macro Precision: 0.9634105178790984\n",
      "Macro Recall: 0.9616666666666666\n",
      "Macro F1: 0.9613662406752049\n",
      "\n",
      "===== Abusive Detection Metrics =====\n",
      "Accuracy: 0.73\n",
      "Precision: 0.7331081081081081\n",
      "Recall: 0.7233333333333334\n",
      "F1-score: 0.7281879194630873\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 1️⃣ Imports\n",
    "# -------------------------------\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# -------------------------------\n",
    "# 2️⃣ Prediction loop on test set\n",
    "# -------------------------------\n",
    "true_langs = []\n",
    "true_abuse = []\n",
    "pred_langs = []\n",
    "pred_abuse = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    text = row['text']\n",
    "    out = predict_abuse_language(text)  # your model inference function\n",
    "\n",
    "    # True labels\n",
    "    true_langs.append(row['lang'])\n",
    "    true_abuse.append(int(row['label']))\n",
    "\n",
    "    # Predicted labels\n",
    "    pred_langs.append(out['predicted_language'])\n",
    "    pred_abuse.append(1 if out['abuse_prediction'] == \"Abusive\" else 0)\n",
    "\n",
    "# -------------------------------\n",
    "# 3️⃣ Language detection metrics\n",
    "# -------------------------------\n",
    "print(\"===== Language Detection Metrics =====\")\n",
    "print(\"Accuracy:\", accuracy_score(true_langs, pred_langs))\n",
    "print(\"Macro Precision:\", precision_score(true_langs, pred_langs, average='macro'))\n",
    "print(\"Macro Recall:\", recall_score(true_langs, pred_langs, average='macro'))\n",
    "print(\"Macro F1:\", f1_score(true_langs, pred_langs, average='macro'))\n",
    "\n",
    "# -------------------------------\n",
    "# 4️⃣ Abusive detection metrics\n",
    "# -------------------------------\n",
    "print(\"\\n===== Abusive Detection Metrics =====\")\n",
    "print(\"Accuracy:\", accuracy_score(true_abuse, pred_abuse))\n",
    "print(\"Precision:\", precision_score(true_abuse, pred_abuse))\n",
    "print(\"Recall:\", recall_score(true_abuse, pred_abuse))\n",
    "print(\"F1-score:\", f1_score(true_abuse, pred_abuse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5c6d8-2eff-4e68-bed2-80e75328f743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68325a70-71e1-4d55-b03d-2b325ec94c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
